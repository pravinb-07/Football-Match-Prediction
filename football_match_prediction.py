# -*- coding: utf-8 -*-
"""Football_Match_Prediction_Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JOnNSSqd6GmYaZMIZdfu2lD7JDDdq6UN
"""

# from google.colab import drive
# drive.mount('/content/drive')

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from datetime import datetime as dt
import itertools
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from mlxtend.plotting import plot_confusion_matrix

#dataTrain=pd.read_csv('/content/drive/MyDrive/Project/final_dataset(1860-0,1,2).csv')
#dataTest=pd.read_csv('/content/drive/MyDrive/Project/test(800-0,1,2).csv')
dataTrain=pd.read_csv('/content/drive/MyDrive/Datasets/final_dataset(1860-0,1,2) (1).csv')
dataTest=pd.read_csv('/content/drive/MyDrive/Datasets/test(800-0,1,2) (1).csv')

batch_size = 64

input_dim = 29

units = 64
output_size = 3 # labels are from Win or Loss or Draw


def build_model(allow_cudnn_kernel=True):
    
    if allow_cudnn_kernel:
        # The LSTM layer with default options uses CuDNN.
        lstm_layer = keras.layers.LSTM(units, input_shape=(input_dim,1))
    else:
        # Wrapping a LSTMCell in a RNN layer will not use CuDNN.
        lstm_layer = keras.layers.RNN(
            keras.layers.LSTMCell(units), input_shape=(input_dim,1)
        )
    model = keras.models.Sequential(
        [
            lstm_layer,
            keras.layers.BatchNormalization(),
            keras.layers.Dense(output_size,activation="softmax"),
        ]
    )
    return model
x_train, y_train = dataTrain.iloc[:,0:29].values,dataTrain.iloc[:,:1].values
#print("y_train")
#print(y_train)
x_train=np.reshape(x_train,(1860,29,1))
#x_train=StandardScaler()scaler.fit(X_train)
y_train=keras.utils.to_categorical(y_train,num_classes=3)
#print("y_train")
#print(y_train)
x_test, y_test = dataTest.iloc[:,0:29].values,dataTest.iloc[:,:1].values
#print("y_test")
#print(y_test)
x_test=np.reshape(x_test,(800,29,1))
y_test=keras.utils.to_categorical(y_test,num_classes=3)  
print("y_test")
print(y_test)
model = build_model(allow_cudnn_kernel=True)

model.compile(
    loss=keras.losses.CategoricalCrossentropy(from_logits=True),
    optimizer="Adam",
    metrics=["categorical_accuracy"],
)

model.summary()  
history=model.fit(
    x_train, y_train, validation_split=0.1,batch_size=batch_size, epochs=12
)

# history.history??

print(y_test)

plt.plot(history.history['categorical_accuracy'],color='blue')
plt.plot(history.history['val_categorical_accuracy'],color='green')
plt.title('Model Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend(['Training Accuracy','Validation Accuracy'])
plt.show()

plt.plot(history.history['loss'],color='orange')
plt.plot(history.history['val_loss'],color='red')
plt.title('Model Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend(['Training Loss','Validation Loss'])
plt.show()

#plt.plot(history.history['loss'],color='red')
#plt.title('Model Loss')
#plt.xlabel('Epochs')
#plt.ylabel('Loss')
#plt.legend(['Training Loss'])
#plt.show()

y_pred=model.predict(x_test)
history1=y_pred
y_pred=np.argmax(y_pred,axis=1)
#print(y_pred)
y_test=np.argmax(y_test,axis=1)
#print(y_test)

print(classification_report(y_test, y_pred))
# 0 - Win, 1 - Loss, 2 - Draw

cm=confusion_matrix(y_test,y_pred)
cm

sns.heatmap(cm, annot=True)

#class_names = ['Home Win', 'Away Win', 'Draw']
fig,ax =plot_confusion_matrix(conf_mat=cm)
plt.show()